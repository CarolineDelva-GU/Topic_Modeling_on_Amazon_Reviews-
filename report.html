<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Caroline Delva">
<meta name="author" content="Amina Nsanza">
<meta name="dcterms.date" content="2025-12-12">

<title>Beyond LDA: A Comparative Analysis of Topic Modeling on Amazon Reviews</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="report_files/libs/clipboard/clipboard.min.js"></script>
<script src="report_files/libs/quarto-html/quarto.js"></script>
<script src="report_files/libs/quarto-html/popper.min.js"></script>
<script src="report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="report_files/libs/quarto-html/anchor.min.js"></script>
<link href="report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="report_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="report_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Beyond LDA: A Comparative Analysis of Topic Modeling on Amazon Reviews</h1>
<p class="subtitle lead">Advanced NLP DSAN 5800 · Professor Chris Larson</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Caroline Delva </p>
             <p>Amina Nsanza </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 12, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="i.-introduction" class="level2">
<h2 class="anchored" data-anchor-id="i.-introduction">I. Introduction</h2>
<p>Topic modeling is one of the most widely used techniques in the industry today, from technology to health and marketing; it provides powerful tools that assist in organizing and understanding themes in large unstructured text data. This type of modeling automatically discovers meaningful subjects within documents, and enables businesses to better understand conversational structures, uncover common themes, and gain insights into human behavior. Although topic modeling is the most popular approach to understanding human behavior, there are fundamental differences in the multiple methods of topic modeling, which range from probabilistic and dimensional reduction techniques, embedding-based methods, indicating that there is no single solution that is a one-size-fits-all.</p>
<p>This project explores a comparative analysis of three of the main topic modeling methods today: Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), and a BERT-based transformer model (BERTopic). This comparison is critical because all of these models use fundamentally different methodologies for processing text and uncovering the themes in it. The main objective is to evaluate and compare how well these models perform in discovering meaningful topics from an Amazon Reviews dataset. The performance metrics that will be used to measure success are: Topic coherence, Topic diversity, and Clustering performance through Silhouette score. These performance metrics will assess the models’ ability to capture how meaningful and interpretable each topic is and whether top words within a topic have a logically connected theme. Additionally, they measure how distinct the topics are from one another and if a wide range of themes is captured, rather than redundant and overlapping topics. Finally, the metrics will evaluate how well a model groups similar reviews together and validate that topics reflect real structure in customer feedback.</p>
<hr>
</section>
<section id="ii.-literature-review" class="level1">
<h1>II. Literature Review</h1>
<p>Classic topic modeling, like LDA, represent documents through a “bag-of-words” approach. This framework models each document as a finite mix over latent topics, and then each topic is later on extracted and defined by a distribution over words (1). Although LDA is a foundational model, it is limited because, it disregards semantic relationships and the natural sequential order of words. This classic probabilistic approach will be the baseline model in our project.</p>
<p>​ Attempts were made to bridge LDA’s gap between the order of words and the semantic relationship by introducing methods such as LSA. Where LDA would fall short because of its unreliability of simple keyword matching, LSA uses matrix decomposition to discover the latent structure that was disregarded by LDA in the term-document relationships (2). The LSA model does this by using the Singular Value Decomposition (SVD) method. This matrix decomposition method is used on a term-document matrix that is usually derived from TF-IDF counts, it then uncovers the underlying latent semantic structure in a reduced-dimensional space (2). LSA’s ability to overcome issues such as synonymy, many ways to express a concept, and polysemy, words having multiple meanings, highlights why classical models may not be suited to interpret diverse and nuanced corpuses.</p>
<p>Presently, there are modern contextual topic modeling approaches, such as BERTopic, this model addresses the challenges that were faced by LDA and LSA by completely moving away from the word-frequency statistics method to instead using rich document embeddings based representations (3). The BERTopic model is able to achieve this by using other pretrained language models like the baseline the Sentence-BERT. The language based transformer converts documents into rich dense embeddings. These embeddings from Sentence-Bert are then later used to uncover semantics, this is done through the encoding of semantic meaning. Additionally, BERTopic tops LDA and LSA, because it is able to use the encodings and generate natural clusters where, similar general ideas/themes are pulled together. These idea clusters (topics) are created in a reduced vector space, that is made by UMAP dimensionality reduction, and the clusters are formed through HDBSCAN. To incorporate semantic meaning and topic interpratation, the model uses a class based procedure ( c-TF-IDF) that interprets topic descriptions as well as balances semantics with interpretability (3).</p>
<p>Therefore, through this literature review, we can see that contextual approaches capture more coherent topics; however, they do this by relying on baseline models and offer hybrid structures that differ from single source models. The difference in the methodologies of these models make direct comparison critical. Classic models offer simplicity and efficiency, whereas modern contextual models provide stronger semantic modeling in return; however, they may come at a computational cost. This is why conducting a side by side evaluation is meaningful, so we can obtain a deeper understanding of how each of the models’ approach affect topic quality, coherence, and practical usefulness.</p>
<hr>
<section id="iii.-methods" class="level2">
<h2 class="anchored" data-anchor-id="iii.-methods">III. Methods</h2>
<p>This study was run on an EC2 instance and used an S3 bucket as storage so that the analysis could be automated. Each step retrieved the resulting files from the previous step and dumped the new resulting files into a subfolder of the S3 bucket. That well-aligned data pipeline ensured that the analysis can be reproduced seamlessly without the limitation of local storage.</p>
<section id="a.-data-collection" class="level4">
<h4 class="anchored" data-anchor-id="a.-data-collection">A. Data Collection</h4>
<p>The first step of the pipeline was data collection as the JSON files were scraped from the University of California San Diego Amazon Reviews 2023 website (UCSD Amazon Reviews 2023, 2023). The university’s source index page contained multiple JSON.gz files separated by product category such as all_beauty, disks, movies_and_tv etc. All files were later dumped into the S3 bucket subfolder, amazon_raw/S3 so that the files were not stored locally as each file contained more than 1 million records thus were incredibly large.</p>
</section>
<section id="b.-data-preprocessing" class="level4">
<h4 class="anchored" data-anchor-id="b.-data-preprocessing">B. Data Preprocessing</h4>
<p>The second step of the pipeline was data preprocessing and consisted of the following three steps. By preprocessing the data, noise and bias were eliminated and the reliability of the models’ outputs was ensured.</p>
<section id="data-cleaning" class="level5">
<h5 class="anchored" data-anchor-id="data-cleaning">1. Data Cleaning</h5>
<p>Each JSON file was read from the amazon_raw/ S3 subfolder and their reviewText, summary, and title fields were lowercased, scrubbed of URLs, usernames, non-ASCII characters, punctuation, and extra whitespace. Due to computational constraints, only the all_beauty json file was successfully cleaned. By removing these characters from the text, the models were able to focus on useful as opposed to human level text markers.</p>
</section>
<section id="named-entity-recognition-redaction" class="level5">
<h5 class="anchored" data-anchor-id="named-entity-recognition-redaction">2. Named Entity Recognition Redaction</h5>
<p>NER, which is the NLP process of recognizing known entities from text and replacing them with tags, was then conducted. The model used for this process is spaCy’s en_core_web_sm model. For example, detected people, organizations, locations, and other entity types were replaced with tags like [PERSON], [ORG], and [LOCATION]. By doing so, NER prevents the model from creating topics around specific entities. The clean file was written to a temporary file and then uploaded to the amazon_clean/S3 subfolder.</p>
</section>
</section>
<section id="text-preprocessing" class="level4">
<h4 class="anchored" data-anchor-id="text-preprocessing">3. Text Preprocessing</h4>
<p>Further text preprocessing was conducted in the AmazonPreprocessor class, which was the vectorization class, because more noise appeared when the topics were being generated. The noise elimination included scikit-learn’s token pattern of length less than three, HTML tags such as “br” and “nbsp” and stopwords. The removed stopwords included scikit-learn’s English list and NLTK stopwords. As the analysis was on Amazon reviews, certain words such “great” and “product” were expected from the data. They were therefore removed to assure that meaningful signals were captured from the topics.</p>
</section>
<section id="c.-data-transformation" class="level4">
<h4 class="anchored" data-anchor-id="c.-data-transformation">C. Data Transformation</h4>
<p>The third step of the pipeline was the generation of three types of vectors, count vectors, TF-IDF vectors and sentence embeddings. By transforming the text into these vectors, the comparison of models that rely on the three vector types was possible. The vectorization steps were conducted with a class AmazonPreprocessor that read the clean files from amazon, applied the additional aforementioned preprocessing, and vectorized using three class methods that each performed a type vectorization. After data transformation, the resulting files were stored in the amazon_vectors/S3 subfolder containing the three vectors’ subfolders.</p>
<section id="count-vectorization" class="level5">
<h5 class="anchored" data-anchor-id="count-vectorization">1. Count vectorization</h5>
<p>Count vectorization was conducted using the class build_count_vectors method, which collected the clean files from S3 and fitted a scikit-learn CountVectorizer model to the text. The parameters of the model included the consideration of both unigrams and bigrams, a maximum of 40,000 features, and a frequency filter of (min_df=20, max_df=0.5). By applying these parameters, signal words and two word phrases were considered in the count. The model’s features did not exceed an exorbitant number of features. Only words of moderate frequency were considered as overly frequent and overly rare words were not considered from the corpus. As a result, sparse documents–term matrix and vocabulary mapping were added to the amazon_vectors/countvectorizer/S3 subfolder as vectors.npz and vocab.json.</p>
</section>
<section id="term-frequencyinverse-document-frequency-vectorization" class="level5">
<h5 class="anchored" data-anchor-id="term-frequencyinverse-document-frequency-vectorization">2. Term Frequency–Inverse Document Frequency Vectorization</h5>
<p>Term Frequency-Inverse Document Frequency vectorization was conducted using the build_tfidf_vectors class method which used the same document stream, token pattern, stopwords, ngram_range, and frequency thresholds as the count vectorization model. This step produced a sparse matrix of TF–IDF weights instead of raw counts. As a result, count vectors, the matrix and the vocabulary were then uploaded to S3 under amazon_vectors/tfidf/ for later modeling.</p>
</section>
<section id="sentence-embeddings" class="level5">
<h5 class="anchored" data-anchor-id="sentence-embeddings">3. Sentence embeddings</h5>
<p>Dense sentence embeddings were created using the build_embeddings method by loading the SentenceTransformer all-MiniLM-L6-v2. The model encoded the clean text in batches of 64 into a fixed-size NumPy array of embeddings. The array was saved as embeddings.npy and uploaded to amazon_vectors/embeddings/S3 subfolder.</p>
</section>
</section>
<section id="d.-exploratory-data-analysis" class="level4">
<h4 class="anchored" data-anchor-id="d.-exploratory-data-analysis">D. Exploratory Data Analysis</h4>
<p align="center">
<iframe src="./data/eda_results/plotly_rating_dist.html" width="800" height="500">
</iframe>
</p><figcaption>
Figure 1: Frequency Distribution of Customer Ratings Across Beauty Amazon Reviews
</figcaption>
<p></p>
<p align="center">
<iframe src="./data/eda_results/plotly_text_dist.html" width="800" height="500">
</iframe>
</p><figcaption>
Figure 2: Frequency Distribution of Word Counts Across Beauty Amazon Reviews
</figcaption>
<p></p>
<p align="center">
<iframe src="./data/eda_results/plotly_verified_purchase.html" width="800" height="500">
</iframe>
</p><figcaption>
Figure 3: Distribution of Star Ratings for Verified vs.&nbsp;Non-Verified Purchases
</figcaption>
<p></p>
<p align="center">
<iframe src="./data/eda_results/interactive_reviews_ratings_2023.html" width="800" height="500">
</iframe>
</p><figcaption>
Figure 4: Monthly Trends in Beauty Amazon Reviews and Average Ratings
</figcaption>
<p></p>
</section>
<section id="e.-modeling" class="level4">
<h4 class="anchored" data-anchor-id="e.-modeling">E. Modeling</h4>
<p>The fifth step in the pipeline was the training of the three selected topic models, Latent Dirichlet Allocation, Latent Semantic Analysis, and Bidirectional Encoder Representations from Transformers. The three models used the three types of vectors as input and provided insights on how these models would perform. Each model outputted 10 topics. Graphs were also generated to better understand the result . The outputs of each model were saved in the data/ local folder</p>
<section id="latent-dirichlet-allocation" class="level5">
<h5 class="anchored" data-anchor-id="latent-dirichlet-allocation">1. Latent Dirichlet Allocation</h5>
<p>A Latent Dirichlet Allocation (LDA) model from scikit-learn was fitted to the previously saved All Beauty Amazon reviews count vectors to generate 10 topics. As the literature has established LDA treats each document as a mix of topics, and each topic as a group of words that often appear together <span class="citation" data-cites="blei2003lda">(<a href="#ref-blei2003lda" role="doc-biblioref">Blei, Ng, and Jordan 2003</a>)</span> (Blei, Ng, and Jordan, 2003).</p>
<p>The model’s parameters consisted of batch processing with a random seed for consistency. The 10 topics were considered by the words that received the highest weights. The resulting files were a text file of the generated topics, the trained model and a JSON file of the corpus’ vocabulary. In sum, the directory included count_vectors.npz, vocab.json, lda_topics.txt, and a wordclouds folder. Each set of words were later interpreted as short phrases so that they can be easily understood. Word clouds were generated to better understand the weight of each word in a topic as the words with the highest weights appear bigger than the others. All outputs were saved locally under the data/lda_results folder. These outputs would later be evaluated to determine the LDA model’s performance as compared to the other two models.</p>
</section>
<section id="latent-semantic-analysis" class="level5">
<h5 class="anchored" data-anchor-id="latent-semantic-analysis">2. Latent Semantic Analysis</h5>
<p>Latent Semantic Analysis (LSA), identified lower-dimensional topics in the All Beauty Amazon reviews. LSA applied Truncated SVD to the TFIDF matrix, which reduced sparse text features to dense semantic components (Deerwester et al, 1990). The workflow downloaded the TFIDF vectors and vocabulary from S3 and loaded the sparse matrix locally to keep the feature space aligned with the preprocessing step.</p>
<p>The model used ten topics using scikit learn TruncatedSVD with a fixed random seed. The pipeline applied a Normalizer after SVD to produce stable document embeddings. After training, the code extracted the highest weight terms for each component from the SVD loadings. These terms highlighted the words that define each LSA dimension.</p>
</section>
<section id="bidirectional-encoder-representations-from-transformers-bertopic" class="level5">
<h5 class="anchored" data-anchor-id="bidirectional-encoder-representations-from-transformers-bertopic">3. Bidirectional Encoder Representations from Transformers BERTopic</h5>
<hr>
</section>
</section>
</section>
<section id="iv-results" class="level2">
<h2 class="anchored" data-anchor-id="iv-results">IV Results</h2>
<section id="a.-latent-dirichlet-analysis" class="level3">
<h3 class="anchored" data-anchor-id="a.-latent-dirichlet-analysis">A. Latent Dirichlet Analysis</h3>
<p>The 10 topics generated by the LDA model were consistent with the beauty product theme of the dataset. As the top weighted words were considered as the topics, each set of words can be easily characterized as a beauty product category. For example, as seen in the LDA topics table below, the first topic included words such as “hair”, “iron”, “heat” , which can easily be interpreted as heat styling tools. The second topic included words such as “brush”, “quality”, “bristle”, which can be interpreted as brush set quality. Overall, the topics reflected a very specific category of the beauty theme as some topic were about eye products and the other was about foot products.</p>
<p align="center">
<iframe src="./data/lda_results/ldatable.png" width="800" height="500">
</iframe>
</p><figcaption>
Figure 5: LDA Topic terms and themes for All Beauty reviews
</figcaption>
<p></p>
<p>When considering Topic 0 wordcloud more closely, it was obvious that “hair”, “iron” “clips” and “flat” were the largest in size in that wordcloud; they were the highest weighted terms in that topic. For that reason, it was easily determined that this particular topic could be interpreted as hair heating tools.</p>
<p align="center">
<iframe src="./data/lda_results/wordclouds/topic_0.png" width="800" height="500">
</iframe>
</p><figcaption>
Figure 6: LDA topic 0 wordcloud
</figcaption>
<p></p>
</section>
<section id="b.-latent-semantic-analysis" class="level3">
<h3 class="anchored" data-anchor-id="b.-latent-semantic-analysis">B. Latent Semantic Analysis</h3>
<p>The 10 generated topics from the LSA model were also consistent with the beauty product theme of the dataset. These words were considered the highest signals of the TF-IDF space. However, although the topics were interpreted as phrases, the separation of the beauty categories of the topics were not as distinct as the LDA. For example, the first topic included words such as “hair”, “skin” and “face”, which can place this category in all three categories. The second topic was more consistent with a curly hair care topic. The third topic included words such as “hair”, “skin” and “face”, which can again describe the three categories for beauty products.</p>
<p align="center">
<iframe src="./data/lsa_results/lsatable.png" width="800" height="500">
</iframe>
</p><figcaption>
Figure 7: LDA Topic terms and themes for All Beauty reviews
</figcaption>
<p></p>
<p>Figure 6</p>
<p>The weight distribution of each topic was visualized in the following figure. Between a range of -1 and 1, the weight of each topic indicated how strongly a topic represented the documents that it was reflecting. For example, topic 0 had the highest median weight, which shows that it represented the broadest spread in the vocabulary as compared to the other topics. Some topics show a wider distribution as compared to others, which indicated that some topics were more specialized than others.</p>
<p align="center">
<iframe src="./data/lsa_results/topicdistribution.png" width="800" height="500">
</iframe>
</p><figcaption>
Figure 8: Distribution of LSA Topic Strengths Across All Documents
</figcaption>
<p></p>
</section>
<section id="c.-bertopic" class="level3">
<h3 class="anchored" data-anchor-id="c.-bertopic">C. BERTopic</h3>
<p align="center">
<iframe src="./data/bert_results/visuals/vis_topics_overview.html" width="850" height="600" style="border:none;">
</iframe>
</p><figcaption>
Figure 9: Intertopic Distances in BERTopic Model
</figcaption>
<p></p>
</section>
</section>
<section id="v.-model-performance-comparison" class="level2">
<h2 class="anchored" data-anchor-id="v.-model-performance-comparison">V. Model Performance &amp; Comparison</h2>
<p>Topic Coherence:</p>
<ul>
<li>Assesses how meaningful &amp; interpretable each topic is.</li>
<li>Ensures that the top words within a topic have a logically connected theme.</li>
</ul>
<p>Topic Diversity:</p>
<ul>
<li>Measures how distinct the topics are from one another.</li>
<li>Ensures that a model captures a wide range of themes instead of redundant or overlapping topics.</li>
</ul>
<p>Silhouette Score (Clustering Performance):</p>
<ul>
<li>Evaluates how well the model groups similar reviews together,</li>
<li>Validates that topics reflect real structure in customer feedback.</li>
</ul>
<section id="a.-topic-model-evaluation-comparison" class="level4">
<h4 class="anchored" data-anchor-id="a.-topic-model-evaluation-comparison">A. Topic Model Evaluation Comparison</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 26%">
<col style="width: 18%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Topic Coherence (c_v)</th>
<th>Topic Diversity</th>
<th>Silhouette Score (Cluster Performance)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>LDA</strong></td>
<td>0.3000</td>
<td><strong>0.83</strong></td>
<td><strong>0.4563</strong></td>
</tr>
<tr class="even">
<td><strong>LSA</strong></td>
<td>0.3058</td>
<td>0.47</td>
<td>-0.0265</td>
</tr>
<tr class="odd">
<td><strong>BERTopic</strong></td>
<td><strong>0.4204</strong></td>
<td>0.6384</td>
<td>0.0502</td>
</tr>
</tbody>
</table>
<p>Across all three BERTopic performs best on topic coherence.</p>
<ul>
<li>Discovered themes that are generally more semantically meaningful and internally consistent</li>
<li>Aligns with expectations → relies on dense sentence embeddings instead of raw word counts.</li>
</ul>
<p>On topic diversity, LDA comes out strongest → a wider spread of unique top words across topics.</p>
<ul>
<li>Better at separating themes distinctly, even if the topics themselves aren’t as coherent.</li>
<li>LSA struggled → overlapping, less interpretable topics.</li>
</ul>
<p>For cluster performance, that was measured by silhouette score, LDA again performs the best. - BERTopic’s silhouette score was weak - LSA’s poor score → clusters are weakly formed but directionally sensible.</p>
<p><strong>BERTopic</strong> was the strongest model for <strong>interpretability</strong> and semantic quality, while <strong>LDA</strong> is the strongest for <strong>structural separation</strong> and topic distinctiveness. LSA consistently underperforms across metrics.</p>
</section>
<section id="what-to-model-to-use.." class="level4">
<h4 class="anchored" data-anchor-id="what-to-model-to-use..">WHAT TO MODEL TO USE…..</h4>
<ul>
<li>If you want meaningful, human-like topic themes → <strong>BERTopic</strong>.</li>
<li>If you need sharply separated clusters → <strong>LDA</strong> is better.</li>
</ul>
<p>Overall BERTopic did outperform the baseline models on topic coherence so texts such as reviews, the model would be better and capturing real semantic structure.</p>
<hr>
</section>
</section>
<section id="vi-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="vi-conclusion">VI Conclusion</h2>
<p>The original expectation of this study was that BERT would outperform LDA and LSA given the use of sentence embeddings which captured semantic meaning across the corpus. However, the findings of the current study revealed that the comparison of the topic models’ performance was more layered than originally expected. BERTopic outperformed both models in terms of providing human-like topics; while LDA provided more distinctly separated topics and LSA had the lowest overall performance. Therefore, deciding on which model would perform the best depended mostly on the objective of the topic modeling task as opposed to the model itself. If discovering distinctly separated topics was the main task, LDA would be the choice. If discovering semantically sound topics was the main task, BERT would be the choice. But, it was clear that LSA underperformed for this current study so it would not be a good choice to implement LSA as a way of topic selection.</p>
<p align="center">
<img src="./data/bert_results/visuals/conclusion_flowchart.png" width="700">
</p><figcaption>
Figure 10: Current Study’s Full Pipeline
</figcaption>
<p></p>
<hr>
</section>
<section id="vii-limitations" class="level2">
<h2 class="anchored" data-anchor-id="vii-limitations">VII Limitations</h2>
<p>The main limitation of this study was computation. The current study sought the use of an EC2 instance and S3 bucket; however, the size of the files remained an obstacle as the analysis progressed. With that as the main obstacle, choices were made throughout the study that reflected that roadblock such as focusing on one Amazon review category, decreasing the number of topics generated to 10, and only processing a subset of the reviews during modeling. As a result, the models could not be evaluated on their performance using more than one product category. The question remained would the models provide even better performance if the entire dataset was used for the current study.</p>
<hr>
</section>
<section id="viii-future-works" class="level2">
<h2 class="anchored" data-anchor-id="viii-future-works">VIII Future Works</h2>
</section>
<section id="references" class="level2 unnumbered">


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-blei2003lda" class="csl-entry" role="listitem">
Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. <span>“Latent Dirichlet Allocation.”</span> <em>Journal of Machine Learning Research</em> 3: 993–1022.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>