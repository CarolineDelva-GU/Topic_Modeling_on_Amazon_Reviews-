{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28bc696",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26ac012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import umap\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb38b0b",
   "metadata": {},
   "source": [
    "## Topic Coherence  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082b6e5",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48780bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First topic sample: ['hair', 'fine', 'iron', 'day', 'place', 'work', 'clips', 'stay', 'flat', 'dryer']\n"
     ]
    }
   ],
   "source": [
    "lda_topics = []\n",
    "current_topic = []\n",
    "\n",
    "with open(\"../data/lda_results/lda_topics.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Where the topic starts \n",
    "        if line.startswith(\"Topic\"):\n",
    "            if current_topic:\n",
    "                lda_topics.append(current_topic)\n",
    "                current_topic = []\n",
    "            continue\n",
    "\n",
    "        #skipping the dashed line in txt\n",
    "        if line.startswith(\"-\"):\n",
    "            continue\n",
    "\n",
    "        # extracting the word the first item before each the tab\n",
    "        word = line.split(\"\\t\")[0]\n",
    "        current_topic.append(word)\n",
    "\n",
    "#appending the the final topic\n",
    "if current_topic:\n",
    "    lda_topics.append(current_topic)\n",
    "\n",
    "print(\"First topic sample:\", lda_topics[0][:10])  #checks out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0678c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing & creating gensim dictionary\n",
    "\n",
    "df_5k = pd.read_csv(\"../data/bert_results/bertopic_5k_reviews_with_topics.csv\")  # or your saved CSV\n",
    "documents = df_5k[\"text\"].astype(str).tolist()\n",
    "tokenized_docs = [doc.split() for doc in documents]\n",
    "\n",
    "dictionary = Dictionary(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bd8813c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Coherence (c_v): 0.2999875790610845\n"
     ]
    }
   ],
   "source": [
    "# Getting the topic coherence \n",
    "\n",
    "cm_lda = CoherenceModel(\n",
    "    topics=lda_topics,\n",
    "    texts=tokenized_docs,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "lda_coherence = cm_lda.get_coherence()\n",
    "print(\"LDA Coherence (c_v):\", lda_coherence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0373d370",
   "metadata": {},
   "source": [
    "The LDA model got a coherence score of 0.2999, which is pretty low compared to the other models below. However, this isn't surprising, since LDA relies mostly on word counts and doesn’t really capture the meaning behind what people are saying in the reviews. With beauty products, consumers mostly use varied and subjective language when describing products, so LDA ends up forming topics that are broader and introduces some noise. Although LDA still captured themes, the low coherence score did have challenges with consistently interpretable topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f683b20d",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c639cdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample topic words: ['hair', 'skin', 'work', 'brush', 'color', 'long', 'quality', 'price', 'face', 'dry']\n",
      "LSA Coherence (c_v): 0.3057626280257974\n"
     ]
    }
   ],
   "source": [
    "#Extracting the topic words again for LSA similar code as LDA \n",
    "\n",
    "\n",
    "lsa_topics = []\n",
    "current_topic = []\n",
    "\n",
    "with open(\"../data/lsa_results/lsa_topics.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\"### LSA Topic\"):\n",
    "            if current_topic:\n",
    "                lsa_topics.append(current_topic)\n",
    "                current_topic = []\n",
    "            continue\n",
    "\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        parts = line.split()\n",
    "        word = parts[0]\n",
    "        current_topic.append(word)\n",
    "\n",
    "if current_topic:\n",
    "    lsa_topics.append(current_topic)\n",
    "\n",
    "print(\"Sample topic words:\", lsa_topics[0][:10])\n",
    "\n",
    "\n",
    "# Computing the coherence for LSA using the dictionary and tokenized documents as above of the 5k review \n",
    "\n",
    "\n",
    "cm_lsa = CoherenceModel(\n",
    "    topics=lsa_topics,\n",
    "    texts=tokenized_docs,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "lsa_coherence = cm_lsa.get_coherence()\n",
    "print(\"LSA Coherence (c_v):\", lsa_coherence)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3813b6a",
   "metadata": {},
   "source": [
    "The LSA model reached a coherence score of 0.3057, which is actually solid since the model is simpler than the other methods that compared the embedding-based approach. Since LSA models capture both frequency and word relationships, even if words they don’t fully belong to the same real-world theme, so this leads to topics feeling a bit broader or slightly noisier. \n",
    "\n",
    "Although the score is low, for LSA its a not as bad since it was still able to pick up on some meaningful structure in the reviews, especially around strong, repeated product descriptors like “hair,” “skin,” and “smell.” The overall performance was better than expected but it didn't outperform as well or reach interpretability as the BERTopic did. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1d1f5",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fad5c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERTopic model.\n",
      "Loaded topics: 5000\n"
     ]
    }
   ],
   "source": [
    "# Topic Coherence \n",
    "\n",
    "#Topic Coherence will be evaluating:\n",
    "#How meaningful the top words in each topic are\n",
    "#If the words tend to co-occur in the same context\n",
    "#And +  interpretable the topic is to humans.... We want higher = better (.35 - .55) \n",
    "\n",
    "#Initial assumption: I believe that Bertopic will outperform LDA/LSA here\n",
    "\n",
    "# Loading our previous results \n",
    "\n",
    "# Truncated 5k review subset. \n",
    "df_5k = pd.read_csv(\"../data/bert_results/bertopic_5k_reviews_with_topics.csv\")  # or your saved CSV\n",
    "documents = df_5k[\"text\"].astype(str).tolist()\n",
    "\n",
    "#Bert model \n",
    "topic_model = BERTopic.load(\"../data/bert_results/bertopic_model\")\n",
    "print(\"Loaded BERTopic model.\")\n",
    "\n",
    "#Topic assignments \n",
    "topics = df_5k[\"topic\"].tolist()\n",
    "print(\"Loaded topics:\", len(topics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31872e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic Coherence for Reviews (c_v): 0.4204005495131143\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Top words for ->  each topic\n",
    "topics_words = [\n",
    "    [word for word, _ in topic_model.get_topic(t)]\n",
    "    for t in set(topics)\n",
    "]\n",
    "\n",
    "#Tokenization of the docs. \n",
    "\n",
    "tokenized_docs = [doc.split() for doc in documents]\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "\n",
    "#Getting the c_v coherence -> and will later put this into a table to compare. \n",
    "cm = CoherenceModel(\n",
    "    topics=topics_words,\n",
    "    texts=tokenized_docs,\n",
    "    dictionary=dictionary,  \n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "coherence_score = cm.get_coherence()\n",
    "print(\"BERTopic Coherence for Reviews (c_v):\", coherence_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881b120",
   "metadata": {},
   "source": [
    "The BERTopic model achieved a c_v coherence score of **0.4204**, this indicates an above avagera level of semantic consistency with topics that it extracted. In the context of consumer product reviews, where language can highly subjective, and contain a lot of noise in the data, the modeal was able to capture the topics well and with even with text tha can often be informal. Since coherence scores that range between the 0.35–0.55 range are considered good, this result suggests that BERTopic successfully identified interpretable and meaningful themes across our Amazon Beauty reviews. \n",
    "\n",
    "This scores and the visuals obtaine earlier show that  the top words in each topic shows strong co-occurrence patterns and shared contextual meaning. Overall, the coherence performance confirms that a transformer-based, embedding-driven approach produces well-structured topics that align closely with human interpretations of review content.    ---> Pair these results with the overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b2d062",
   "metadata": {},
   "source": [
    "## Topic Diversity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e41757",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b339ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topic Diversity: 0.83\n"
     ]
    }
   ],
   "source": [
    "# Unique Topic Words Ratio\n",
    "unique_words = set()\n",
    "total_words = 0\n",
    "# correction: use lda_topics\n",
    "for topic_words in lda_topics:\n",
    "    top_words = topic_words[:10] \n",
    "    total_words += len(top_words)\n",
    "    unique_words.update(top_words)\n",
    "\n",
    "# Printing the results\n",
    "lda_topic_diversity = len(unique_words) / total_words\n",
    "print(\"LDA Topic Diversity:\", lda_topic_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d1cdb",
   "metadata": {},
   "source": [
    "The LDA model achieved a really high topic diversity score of 0.83. This shows that the top words across topics are mostly unique and do not repeat very often. This means that LDA is spreading its vocab widely instead of just clustering around the same or similar terms. This can be positive because  the model is capturing  a wide range of themes in the reviews, although this was a hindrance in the performance of earlier in the metrics, this was a sign that the topics are a bit too broad or loosely defined, which lines up with the lower coherence we saw earlier. So while LDA is diverse in and it covers a lot of different words, that doesn’t always translate into cleaner or more meaningful topics compared to the embedding approach like the BERTtopic below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65038a42",
   "metadata": {},
   "source": [
    "### LSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68114821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Topic Diversity: 0.47\n"
     ]
    }
   ],
   "source": [
    "#Topic Diversity for LSA. \n",
    "\n",
    "for topic_words in lsa_topics:\n",
    "    top_words = topic_words[:10]   \n",
    "    total_words += len(top_words)\n",
    "    unique_words.update(top_words)\n",
    "\n",
    "lsa_topic_diversity = len(unique_words) / total_words\n",
    "print(\"LSA Topic Diversity:\", lsa_topic_diversity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee8a9f",
   "metadata": {},
   "source": [
    "The LSA model reached a topic diversity score of 0.47; this score is in the moderate range, since methods like LSA often end up reusing strong descriptive words and do so across multiple topics, so its themes naturally overlap more than those from LDA. A score such as 0.47 suggests that even though LSA does capture different angles of beauty review data, many of the topics share a similar vocabulary, so this can make them feel less distinct. From the score, we can see that LSA struggled a bit to produce clearly separated and interpretable topics compared to the LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c5b016",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316ff79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic Topic Diversity: 0.6383561643835617\n"
     ]
    }
   ],
   "source": [
    "# Topic Diversity \n",
    "\n",
    "#High topic diversity = the model is capturing many different themes rather than repeating the same idea in multiple topics.\n",
    "\n",
    "#Typical ranges: 0.50 & below poor and topic usually repeats Above .70 is good the middle is moderate\n",
    "\n",
    "#.70–0.90 → High diversity (excellent)\n",
    "# Initial assumption: I believe that BERTopic will outperform LDA/LSA here as well.\n",
    "\n",
    "# Using the top 10 words per  -> (This is uniform) \n",
    "u_words = set()\n",
    "t_words = 0\n",
    "\n",
    "for topic_id in set(topics):\n",
    "    top_words = [word for word, _ in topic_model.get_topic(topic_id)[:10]]\n",
    "    t_words += len(top_words)\n",
    "    u_words.update(top_words)\n",
    "\n",
    "topic_dei = len(u_words) / t_words\n",
    "print(\"BERTopic Topic Diversity:\", topic_dei)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a42a4c",
   "metadata": {},
   "source": [
    "The model achieved a topic diversity score of  0.638 , which suggest that the topics are fairly distinct from one another and not overly repetitive. Even though it’s not extremely high, it still showcases the model's ability to capture a wide range of themes from the reviews. This level of diversity means users can get more varied insights across topics, even if a few of them slightly overlap in meaning.  ---> Pair these results with the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d31341e",
   "metadata": {},
   "source": [
    "## Silhouette Score (Clustering Performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c2596",
   "metadata": {},
   "source": [
    "### LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d085cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Silhouette Score: 0.4563104671340692\n"
     ]
    }
   ],
   "source": [
    "# Using the LDA model \n",
    "lda_model = joblib.load(\"../data/lda_results/lda_model.joblib\")\n",
    "# loading count verctors again \n",
    "count_vectors = load_npz(\"../data/lda_results/count_vectors.npz\")\n",
    "\n",
    "#Converting -> array\n",
    "X = count_vectors.toarray()\n",
    "\n",
    "lda_vectors = lda_model.transform(X)   # shape: (n_docs, n_topics)\n",
    "\n",
    "#Assigning the each doc top strongest topic \n",
    "lda_labels = np.argmax(lda_vectors, axis=1)\n",
    "\n",
    "#Computing the score. \n",
    "sil_score = silhouette_score(lda_vectors, lda_labels)\n",
    "\n",
    "print(\"LDA Silhouette Score:\", sil_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71fd4e7",
   "metadata": {},
   "source": [
    "The LDA model ended up with a silhouette score of 0.456; the score is actually much higher than we had anticipated. This suggests that the LDA topics were quite well-separated and the documents mostly clustered pretty consistently around their dominant topic. Sometimes LDA can struggle with overlapping language, in this case, it managed to form groups that were distinct enough for the silhouette score to reflect meaningful structure in the reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c03b8",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f2c772c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Silhouette Score: -0.026518421452788203\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load LSA document-topic vectors\n",
    "lsa_vectors = load_npz(\"../data/lsa_results/vectors.npz\").toarray()\n",
    "\n",
    "# Assign each document to the topic with highest weight\n",
    "lsa_labels = np.argmax(lsa_vectors, axis=1)\n",
    "\n",
    "# Compute Silhouette Score\n",
    "lsa_silhouette = silhouette_score(lsa_vectors, lsa_labels)\n",
    "print(\"LSA Silhouette Score:\", lsa_silhouette)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945fb8de",
   "metadata": {},
   "source": [
    "The LSA model achieved a remarkably low silhouette score of -0.026. At first, it was alarming, but LSA tends to intertwine patterns in the data, so documents didn't get clustered into clean, distinct groups. This is because the distance in the documents is very little, so the topics will not be really distinct but rather overlap each other. LSA isn't necessarily a bad model, but rather it captures wider and more blended topics rather than clean, separated ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb08d1",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034c9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the og embeddings \n",
    "#Og Embeddings  Deleted them will add them if needed. \n",
    "embeddings = np.load(\"../cleaned_data/embeddings/embeddings.npy\")\n",
    "#print(\"Loaded embeddings:\", embeddings.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a4578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 5000 Filtered: 3692\n",
      "BERTopic Silhouette Score: 0.05022962763905525\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#extracting the topic labels\n",
    "labels = np.array(topics)\n",
    "\n",
    "# Removing any outlier  outliers (-1) -> none from the visuals. but just incase. \n",
    "mask = labels != -1\n",
    "filtered_embeddings = embeddings[mask]\n",
    "filtered_labels = labels[mask]\n",
    "\n",
    "print(\"Original:\", len(labels), \"Filtered:\", len(filtered_labels))\n",
    "\n",
    "score = silhouette_score(filtered_embeddings, filtered_labels)\n",
    "print(\"BERTopic Silhouette Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969a2a2",
   "metadata": {},
   "source": [
    "For clustering performance, the BERTopic model gave a silhouette score of 0.05 after removing outliers. This isn’t surprising for beauty review data since people describe products in all kinds of ways, and the themes naturally overlap. So even though the clusters aren’t super tight or clearly separated in the embedding space, the model still manages to pull out meaningful patterns. When you look at this together with the coherence and diversity results, it shows that BERTopic can still capture useful, real-world topics even if the underlying clusters are a bit loose, which honestly reflects how messy human-written reviews usually are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd97aa",
   "metadata": {},
   "source": [
    "## Performance Metric Comparison Table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd9ed6",
   "metadata": {},
   "source": [
    "## Topic Model Evaluation Comparison\n",
    "\n",
    "| Model     | Topic Coherence (c_v) | Topic Diversity | Silhouette Score (Cluster Performance) |\n",
    "|-----------|------------------------|-----------------|----------------------------------------|\n",
    "| **LDA**        | 0.3000                 | **0.83**           | **0.4563**                               |\n",
    "| **LSA**        | 0.3058                 | 0.47            | -0.0265                                 |\n",
    "| **BERTopic**   | **0.4204**             | 0.6384         | 0.0502                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff7122",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
