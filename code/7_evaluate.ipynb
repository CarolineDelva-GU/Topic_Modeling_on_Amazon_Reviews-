{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28bc696",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dsan6000/Topic_Modeling_on_Amazon_Reviews-/nlp_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import umap\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb38b0b",
   "metadata": {},
   "source": [
    "## Topic Coherence  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1d1f5",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad5c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERTopic model.\n",
      "Loaded topics: 5000\n"
     ]
    }
   ],
   "source": [
    "# Topic Coherence \n",
    "\n",
    "#Topic Coherence will be evaluating:\n",
    "#How meaningful the top words in each topic are\n",
    "#If the words tend to co-occur in the same context\n",
    "#And +  interpretable the topic is to humans.... We want higher = better (.35 - .55) \n",
    "\n",
    "#Initial assumption: I believe that Bertopic will outperform LDA/LSA here\n",
    "\n",
    "# Loading our previous results \n",
    "\n",
    "# Truncated 5k review subset. \n",
    "df_5k = pd.read_csv(\"../data/bert_results/bertopic_5k_reviews_with_topics.csv\")  # or your saved CSV\n",
    "documents = df_5k[\"text\"].astype(str).tolist()\n",
    "\n",
    "#Bert model \n",
    "topic_model = BERTopic.load(\"../data/bert_results/bertopic_model\")\n",
    "print(\"Loaded BERTopic model.\")\n",
    "\n",
    "#Topic assignments \n",
    "topics = df_5k[\"topic\"].tolist()\n",
    "print(\"Loaded topics:\", len(topics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31872e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic Coherence for Reviews (c_v): 0.4204005495131143\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "#Top words for ->  each topic\n",
    "topics_words = [\n",
    "    [word for word, _ in topic_model.get_topic(t)]\n",
    "    for t in set(topics)\n",
    "]\n",
    "\n",
    "#Tokenization of the docs. \n",
    "\n",
    "tokenized_docs = [doc.split() for doc in documents]\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "\n",
    "#Getting the c_v coherence -> and will later put this into a table to compare. \n",
    "cm = CoherenceModel(\n",
    "    topics=topics_words,\n",
    "    texts=tokenized_docs,\n",
    "    dictionary=dictionary,  \n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "\n",
    "coherence_score = cm.get_coherence()\n",
    "print(\"BERTopic Coherence for Reviews (c_v):\", coherence_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881b120",
   "metadata": {},
   "source": [
    "The BERTopic model achieved a c_v coherence score of **0.4204**, this indicates an above avagera level of semantic consistency with topics that it extracted. In the context of consumer product reviews, where language can highly subjective, and contain a lot of noise in the data, the modeal was able to capture the topics well and with even with text tha can often be informal. Since coherence scores that range between the 0.35–0.55 range are considered good, this result suggests that BERTopic successfully identified interpretable and meaningful themes across our Amazon Beauty reviews. \n",
    "\n",
    "This scores and the visuals obtaine earlier show that  the top words in each topic shows strong co-occurrence patterns and shared contextual meaning. Overall, the coherence performance confirms that a transformer-based, embedding-driven approach produces well-structured topics that align closely with human interpretations of review content.    ---> Pair these results with the overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b2d062",
   "metadata": {},
   "source": [
    "## Topic Diversity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c5b016",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316ff79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic Topic Diversity: 0.6383561643835617\n"
     ]
    }
   ],
   "source": [
    "# Topic Diversity \n",
    "\n",
    "#High topic diversity = the model is capturing many different themes rather than repeating the same idea in multiple topics.\n",
    "\n",
    "#Typical ranges: 0.50 & below poor and topic usually repeats Above .70 is good the middle is moderate\n",
    "\n",
    "#.70–0.90 → High diversity (excellent)\n",
    "# Initial assumption: I believe that BERTopic will outperform LDA/LSA here as well.\n",
    "\n",
    "# Using the top 10 words per  -> (This is uniform) \n",
    "u_words = set()\n",
    "t_words = 0\n",
    "\n",
    "for topic_id in set(topics):\n",
    "    top_words = [word for word, _ in topic_model.get_topic(topic_id)[:10]]\n",
    "    t_words += len(top_words)\n",
    "    u_words.update(top_words)\n",
    "\n",
    "topic_dei = len(u_words) / t_words\n",
    "print(\"BERTopic Topic Diversity:\", topic_dei)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a42a4c",
   "metadata": {},
   "source": [
    "The model achieved a topic diversity score of **0.638**, which suggest that the topics are fairly distinct from one another and not overly repetitive. Eventhough it’s not extremely high, it still showcases the model's ability to capture a wide range of themes from the reviews. This level of diversity means users can get more varried insights across topics, even if a few of them slightly overlap in meaning.  ---> Pair these results with the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d31341e",
   "metadata": {},
   "source": [
    "## Silhouette Score (Clustering Performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb08d1",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034c9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the og embeddings \n",
    "#Og Embeddings  Deleted them will add them if needed. \n",
    "embeddings = np.load(\"../cleaned_data/embeddings/embeddings.npy\")\n",
    "#print(\"Loaded embeddings:\", embeddings.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c1a4578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 5000 Filtered: 3692\n",
      "BERTopic Silhouette Score: 0.05022962763905525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "#extracting the topic labels\n",
    "labels = np.array(topics)\n",
    "\n",
    "# Removing any outlier  outliers (-1) -> none from the visuals. but just incase. \n",
    "mask = labels != -1\n",
    "filtered_embeddings = embeddings[mask]\n",
    "filtered_labels = labels[mask]\n",
    "\n",
    "print(\"Original:\", len(labels), \"Filtered:\", len(filtered_labels))\n",
    "\n",
    "score = silhouette_score(filtered_embeddings, filtered_labels)\n",
    "print(\"BERTopic Silhouette Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969a2a2",
   "metadata": {},
   "source": [
    "For clustering performance, the BERTopic model gave a silhouette score of 0.05 after removing outliers. This isn’t surprising for beauty review data since people describe products in all kinds of ways, and the themes naturally overlap. So even though the clusters aren’t super tight or clearly separated in the embedding space, the model still manages to pull out meaningful patterns. When you look at this together with the coherence and diversity results, it shows that BERTopic can still capture useful, real-world topics even if the underlying clusters are a bit loose, which honestly reflects how messy human-written reviews usually are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd97aa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
