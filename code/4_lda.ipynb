{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "69b14909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bucket: amazonreviewsnlp\n",
      "Saving results to: /home/ubuntu/Topic_Modeling_on_Amazon_Reviews-/data/lda_results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# S3 config\n",
    "BUCKET_NAME = os.environ.get(\"BUCKET_NAME\", \"amazonreviewsnlp\")\n",
    "VECTORS_KEY = \"amazon_vectors/countvectorizer/vectors.npz\"\n",
    "VOCAB_KEY   = \"amazon_vectors/countvectorizer/vocab.json\"\n",
    "\n",
    "# Local output directory for LDA results\n",
    "RESULTS_DIR = Path(\"../data/lda_results\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "print(\"Using bucket:\", BUCKET_NAME)\n",
    "print(\"Saving results to:\", RESULTS_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e4815a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3 numpy scipy scikit-learn sentence-transformers wordcloud matplotlib ipykernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7aeaed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading count vectors from S3...\n",
      "Downloading vocabulary from S3...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "local_vectors_path = RESULTS_DIR / \"count_vectors.npz\"\n",
    "local_vocab_path   = RESULTS_DIR / \"vocab.json\"\n",
    "\n",
    "print(\"Downloading count vectors from S3...\")\n",
    "s3.download_file(BUCKET_NAME, VECTORS_KEY, str(local_vectors_path))\n",
    "\n",
    "print(\"Downloading vocabulary from S3...\")\n",
    "s3.download_file(BUCKET_NAME, VOCAB_KEY, str(local_vocab_path))\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa42a014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sparse matrix...\n",
      "Matrix shape: (10000, 1756)\n",
      "Vocab size: 1756\n",
      "Sample terms: ['ability', 'able', 'absolutely', 'absorb', 'absorbed', 'absorbed skin', 'absorbs', 'absorbs quickly', 'accessories', 'accidentally', 'accurate', 'acid', 'acne', 'acrylic', 'action', 'actual', 'actually', 'add', 'added', 'adding']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy import sparse\n",
    "\n",
    "print(\"Loading sparse matrix...\")\n",
    "X = sparse.load_npz(local_vectors_path)\n",
    "print(\"Matrix shape:\", X.shape)\n",
    "\n",
    "with open(local_vocab_path) as f:\n",
    "    vocab = json.load(f)\n",
    "inv_vocab = {int(idx): term for term, idx in vocab.items()}\n",
    "\n",
    "feature_names = [inv_vocab[i] for i in range(len(inv_vocab))]\n",
    "\n",
    "print(\"Vocab size:\", len(feature_names))\n",
    "print(\"Sample terms:\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28743321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA with 10 topics...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_topics = 10    \n",
    "max_iter = 10    \n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    max_iter=max_iter,\n",
    "    learning_method=\"batch\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,            \n",
    ")\n",
    "\n",
    "print(\"Fitting LDA with\", n_topics, \"topics...\")\n",
    "lda.fit(X)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved topics to: /home/ubuntu/Topic_Modeling_on_Amazon_Reviews-/data/lda_results/lda_topics.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_top_words = 20\n",
    "topics_txt_path = RESULTS_DIR / \"lda_topics.txt\"\n",
    "\n",
    "def get_topic_terms(lda_model, feature_names, n_top_words=20):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_terms = [(feature_names[i], float(topic[i])) for i in top_indices]\n",
    "        topics.append(top_terms)\n",
    "    return topics\n",
    "\n",
    "topics = get_topic_terms(lda, feature_names, n_top_words=n_top_words)\n",
    "\n",
    "with open(topics_txt_path, \"w\") as f:\n",
    "    for k, topic in enumerate(topics):\n",
    "        f.write(f\"Topic {k}\\n\")\n",
    "        f.write(\"--------------------\\n\")\n",
    "        for term, weight in topic:\n",
    "            f.write(f\"{term}\\t{weight:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"Saved topics to:\", topics_txt_path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9cf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved wordcloud for Topic 0 to ../data/lda_results/wordclouds/topic_0.png\n",
      "Saved wordcloud for Topic 1 to ../data/lda_results/wordclouds/topic_1.png\n",
      "Saved wordcloud for Topic 2 to ../data/lda_results/wordclouds/topic_2.png\n",
      "Saved wordcloud for Topic 3 to ../data/lda_results/wordclouds/topic_3.png\n",
      "Saved wordcloud for Topic 4 to ../data/lda_results/wordclouds/topic_4.png\n",
      "Saved wordcloud for Topic 5 to ../data/lda_results/wordclouds/topic_5.png\n",
      "Saved wordcloud for Topic 6 to ../data/lda_results/wordclouds/topic_6.png\n",
      "Saved wordcloud for Topic 7 to ../data/lda_results/wordclouds/topic_7.png\n",
      "Saved wordcloud for Topic 8 to ../data/lda_results/wordclouds/topic_8.png\n",
      "Saved wordcloud for Topic 9 to ../data/lda_results/wordclouds/topic_9.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wordcloud_dir = RESULTS_DIR / \"wordclouds\"\n",
    "wordcloud_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for k, topic in enumerate(topics):\n",
    "    freq_dict = {term: weight for term, weight in topic}\n",
    "\n",
    "    wc = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color=\"white\"\n",
    "    ).generate_from_frequencies(freq_dict)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Topic {k}\", fontsize=16)\n",
    "\n",
    "    out_path = wordcloud_dir / f\"topic_{k}.png\"\n",
    "    plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved wordcloud for Topic {k} to {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
